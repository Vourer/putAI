{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tworzenie zasobów + wykrywanie encji nazwanych\n",
    "\n",
    "Algorytmy wykorzystywane w problemach przetwarzania języka naturalnego opierają najczęściej swoje działanie o analizę dużych korpusów danych. O ile w zadaniach konkursowych często odpowiednie dane są już przygotowane, o tyle tworząc własne eksperymenty, często musimy sami pozyskać dane i przetransformować do użytecznej postaci.\n",
    "\n",
    "Dzisiejsze laboratoria dotyczyć będą tworzenia korpusów danych, tworzenia reprezentacji CoNNL i wykorzystania jej do zadania wykrywania encji nazwanych.\n",
    "\n",
    "## Automatyczne pozyskiwanie surowych danych tekstowych\n",
    "Dotychczas omawiane metody działały na surowym tekście, który transformowany był do odpowiedniej reprezentacji wektorowej (Bag of words, bag of ngrams, embeddingi). Jak zautomatyzować pozyskiwanie takich surowych danych z internetu?\n",
    "\n",
    "W tej części skupimy się na stworzeniu automatycznego pobieracza danych, który działać będzie w dwóch \"obszarach\":\n",
    "<ol>\n",
    "<li>crawler: moduł odwiedzający kolejne strony internetowy</li>\n",
    "<li>scraper: moduł ekstrahujący treść z konkretnych stron internetowych</li>\n",
    "</ol>\n",
    "\n",
    "Wykorzystajmy do tego dwie biblioteki: \n",
    "\n",
    "**urllib** - do odwiedzania stron\n",
    "\n",
    "**BeautifulSoup** - do parsowania danych (np. w formacie HTML).\n",
    "\n",
    "## Zadanie1 (2pkt): Napisz prosty ekstraktor danych ze stron WWW odwiedzający kilka podstron\n",
    "Ekstraktor ma odwiedzić zadaną stronę internetową, pobrać zawartość wszystkich tekstów wewnątrz paragrafów (wewnątrz tagów P zawartych w pobranym dokumencie HTML), a następnie odwiedzić 5 dowolnych linków z tej strony i z nich analogicznie pobrać zawartość.\n",
    "Łącznie powinniśmy otrzymać dane z 6 adresów internetowch (strona główna + 5 linków ze strony głównej).\n",
    "\n",
    "Do napisania crawlera przydać się mogą następujące funkcje:\n",
    "\n",
    "urllib.request.urlopen() - do pobrania zawartości strony\n",
    "findAll() na obiekcie BeautifulSoup, można ją wykorzystać do przeiterowania po wszystkich tagach danego rodzaju\n",
    "get_text() - Istnieje duża szansa, że wewnątrz tagów P znajdą się również inne tagi HTML, chcielibyśmy oczyścić \n",
    "z nich tekst. Można to zrobić albo z wyrażeniami regularnymi (robiliśmy takie zadanie na pierwszych laboratoriach!), albo użyć właśnie funkcji get_text() z BeautifulSoup\n",
    "\n",
    "Linki do dokumentacji:\n",
    "urllib, pobieranie danych: https://docs.python.org/3/howto/urllib2.html\n",
    "beautifulSoup: https://www.crummy.com/software/BeautifulSoup/bs4/doc/ (przeczytanie QuickStart jest wystarczające do zrobienia tego zadania)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odwiedzam link: http://2020.poleval.pl\n",
      "Odwiedzam link: https://en.wikipedia.org/wiki/SemEval\n",
      "Odwiedzam link: http://2020.poleval.pl\n",
      "Odwiedzam link: http://2019.poleval.pl\n",
      "Odwiedzam link: http://2018.poleval.pl\n",
      "PolEval is a SemEval-inspired evaluation campaign for natural language processing tools for Polish. Submitted tools compete against one another within certain tasks selected by organizers, using available data and are evaluated according to pre-established procedures.PolEval is a SemEval-inspired evaluation campaign for natural language processing tools for Polish. Submitted tools compete against one another within certain tasks selected by organizers, using available data and are evaluated according to pre-established procedures.\n",
      "SemEval (Semantic Evaluation) is an ongoing series of evaluations of computational semantic analysis systems; it evolved from the Senseval word sense evaluation series. The evaluations are intended to explore the nature of meaning in language. While meaning is intuitive to humans, transferring those intuitions to computational analysis has proved elusive.\n",
      "\n",
      "PolEval is a SemEval-inspired evaluation campaign for natural language processing tools for Polish. Submitted tools compete against one another within certain tasks selected by organizers, using available data and are evaluated according to pre-established procedures.\n",
      "PolEval is a SemEval-inspired evaluation campaign for natural language processing tools for Polish. Submitted tools compete against one another within certain tasks selected by organizers, using available data and are evaluated according to pre-established procedures.\n",
      "PolEval is a SemEval-inspired evaluation campaign for natural language processing tools for Polish. Submitted tools compete against one another within certain tasks selected by organisers, using available data and are evaluated according to pre-established procedures.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_html(address: str):\n",
    "    req = urllib.request.Request(address)\n",
    "    try:\n",
    "        response = urllib.request.urlopen(req)\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(e.code)\n",
    "        print(e.read())\n",
    "        return b''\n",
    "    return response.read()\n",
    "\n",
    "\n",
    "def get_soup(address: str):\n",
    "    html = get_html(address)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "\n",
    "def scrap(soup):\n",
    "    return soup.find('p').get_text()\n",
    "\n",
    "\n",
    "def crawl_through(address: str):\n",
    "    text = \"\"\n",
    "    completed = 0\n",
    "    main_soup = get_soup(address)\n",
    "    text += scrap(main_soup)\n",
    "    \n",
    "    for link in main_soup.find_all('a'): \n",
    "        if completed == 5:\n",
    "            break\n",
    "        sub_address = link.get('href')\n",
    "        if sub_address[:4] != 'http':\n",
    "            continue\n",
    "        print(f\"Odwiedzam link: {sub_address}\")\n",
    "        text += scrap(get_soup(sub_address)) + '\\n'\n",
    "        completed += 1\n",
    "    \n",
    "    return text\n",
    "    \n",
    "    \n",
    "print(crawl_through('http://poleval.pl/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2 - CONLL\n",
    "Dane ustrukturyzowane w formacie CONLL.\n",
    "\n",
    "Niektóre algorytmy korzystają z dodatkowych metadanych opisujących poszczególne tokeny (słowa). Bardzo popularnym formatem zapisu takich danych jest format CONLL. \n",
    "\n",
    "Reprezentacja CONLL polega na tym, że dany tekst dzielony jest na zdania, a następnie każde zdanie dzielone jest na tokeny (tokenizowane). Następnie dla każdego tokenu tworzymy listę opisującą cechy tego tokenu (słowa).\n",
    "Poniżej przykład wektora opisującego każdy token zadanego tekstu:\n",
    "<ol>\n",
    "    <li>ID - numer porządkowy tokenu w zdaniu</li>\n",
    "    <li>text - tekst tokenu w formie nieprzetworzonej</li>\n",
    "    <li>Part of Speech tag (POS tag) - informacja o części mowy, która powiązana jest z tym słowem </li>\n",
    "    <li>is digit - flaga (o wartościach 0 lub 1), która informuje nas czy dany token jest liczbą</li>\n",
    "    <li>is punct - flaga (o wartościach 0 lub 1), która informuje nas czy dany token jest znakiem interpunkcyjnym</li>\n",
    "</ol>\n",
    "\n",
    "Wektory cech dla kolejnych słów zapisywane są pod sobą. **Separatorem cech w wektorze jest pojedyncza spacja.**\n",
    "\n",
    "**Zdania zwyczajowo oddzielamy od siebie podwójnym znakiem nowej linii.**\n",
    "\n",
    "Historycznie CONLL był bardzo konkretnym formatem danych w którym mieliśmy z góry narzucone cechy (np. format CONLL-U https://universaldependencies.org/docs/format.html). Liczba cech ewoluowała jednak w czasie i w wielu miejscach CONLL stał się synonimem ogólnego formatu, w którym dobór cech zależy tylko od nas, jednak stałym jest zapis sekwencji tokenów jako sekwencji wierszy w tekście, gdzie każdy wiersz jest listą oddzielonych spacją wartości (cech), a zdania oddzielone są od siebie podwójnym znakiem nowej linii.\n",
    "\n",
    "\n",
    "### Przykład:\n",
    "\n",
    "Tekst: Kasia kupiła 2 lizaki: truskawkowy i zielony. Kasia używa Apple IPhone 5 i IPad.\n",
    "\n",
    "Reprezentacja CONLL **(spacje separujące kolumny zostały zwielokrotnione na potrzeby zwiększenia czytelności)**\n",
    "<pre>\n",
    "1 Kasia  RZECZOWNIK 0 0\n",
    "2 kupiła CZASOWNIK  0 0\n",
    "3 2      LICZEBNIK  1 0\n",
    "4 lizaki RZECZOWNIK 0 0\n",
    "5 .      _          0 1\n",
    "\n",
    "1 Kasia  RZECZOWNIK 0 0\n",
    "2 używa  CZASOWNIK  0 0\n",
    "3 Apple  RZECZOWNIK 0 0\n",
    "4 IPhone RZECZOWNIK 0 0\n",
    "5 5      LICZEBNIK  1 0\n",
    "6 i      SPÓJNIK    0 0\n",
    "7 iPad   RZECZOWNIK 0 0\n",
    "8 .      _          0 1\n",
    "</pre>\n",
    "\n",
    "**Zadanie 2a (0.5 pkt)**: Napisz funkcję, która z zadanego tekstu w formie surowego tekstu stworzy reprezentację CONLL opisaną wcześniej wymienionymi atrybutami (ID, text, POS-tag, is_digit, is_punct).\n",
    "\n",
    "Wykorzystaj sentence splitter i tokenizator z NLTK. Do uzyskania informacji o POS-tagach każdego tokenu wykorzystaj funkcję nltk.pos_tag(). W kolumnie związanej z POS-tagiem zapisz pos tag w takiej formie, w jakiej uzyskamy go z funkcji pos_tag (pos_tag() zwraca formy skrótowe, np. 'NN' dla rzeczowników), nie trzeba więc zamieniać napisu \"NN\" na \"RZECZOWNIK\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID\tText\tPOStag\tis_num\tis_punct\n",
      "1\tKate\tNNP\t0\t0\n",
      "2\tuses\tVBZ\t0\t0\n",
      "3\tIPhone\tNNP\t0\t0\n",
      "4\t5\tCD\t1\t0\n",
      "5\tand\tCC\t0\t0\n",
      "6\tIPad\tNNP\t0\t0\n",
      "7\t.\t.\t0\t1\n",
      "8\tKate\tNNP\t0\t0\n",
      "9\tbought\tVBD\t0\t0\n",
      "10\t2\tCD\t1\t0\n",
      "11\tlolis\tNNS\t0\t0\n",
      "12\t.\t.\t0\t1\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "def generate_conll(text):\n",
    "    postags = pos_tag(word_tokenize(text))\n",
    "    print(f\"ID\\tText\\tPOStag\\tis_num\\tis_punct\")\n",
    "    for i, (word, tag) in enumerate(postags):\n",
    "        print(f\"{i+1}\\t{word}\\t{tag}\\t{1 if tag == 'CD' else 0}\\t{1 if tag == '.' else 0}\")\n",
    "\n",
    "\n",
    "generate_conll(\"Kate uses IPhone 5 and IPad. Kate bought 2 lolis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Wyobraźmy sobie teraz, że chcielibyśmy wykrywać wzmianki o urządzeniach elektronicznych w tekście. W jaki sposób zakodować informację o (potencjalnie wielotokenowych) nazwach produktów w CONLL, tak, aby później móc wykonać proces uczenia?\n",
    "\n",
    "Dodajmy w naszym CONLLu dodatkową kolumnę reprezentującą informację o urządzeniach elektronicznych.\n",
    "Nazwy urządzeń mogą składać się potencjalnie z wielu słów.\n",
    "Do zakodowania wielotokenowych tekstów używa się najczęściej notacji IOB, gdzie każda literka skrótu oznacza interpretację aktualnego słowa:\n",
    "<ul>\n",
    "    <li> B = begin, marker, który mówi, że aktualne słowo to początek nazwy </li>\n",
    "    <li> I = inside, marker, który mówi, że aktualne słowo to kontynacja nazwy, która rozpoczyna się wystąpieniem wcześniejszego B</li>\n",
    "    <li> O = outside, marker, który mówi, że aktualne słowo nie jest interesującą nas nazwą (jest poza nią) </li>\n",
    "</ul>\n",
    "\n",
    "Po dodaniu nowej kolumny (na końcu) nasz CONLL przybiera postać:\n",
    "\n",
    "<pre>\n",
    "1 Kasia  RZECZOWNIK 0 0 O\n",
    "2 kupiła CZASOWNIK  0 0 O\n",
    "3 2                 1 0 O\n",
    "4 lizaki RZECZOWNIK 0 0 O\n",
    "5 .      _          0 1 O\n",
    "\n",
    "1 Kasia  RZECZOWNIK 0 0 O\n",
    "2 używa             0 0 O\n",
    "3 Apple  RZECZOWNIK 0 0 B\n",
    "4 IPhone RZECZOWNIK 0 0 I\n",
    "5 5                 1 0 I\n",
    "6 i      SPÓJNIK    0 0 O\n",
    "7 iPad   RZECZOWNIK 0 0 B\n",
    "8 .      _          0 1 0\n",
    "</pre>\n",
    "\n",
    "Zwróćcie Państwo uwagę na ostatnią kolumnę, czytając tekst od góry w dół, wystąpienie literki \"B\" oznacza początek interesującej frazy (Apple), jeśli zaraz za \"B\" pojawia się sekwencja oznaczona jako \"I\" - kolejne tokeny stanowią kontynuację interesującej nas frazy, w tym przypadku 3 tokeny \"Apple IPhone 5\" tworzą jeden byt. Poza tym widzimy, że \"iPad\" stanowi osobny, jednotokenowy byt.\n",
    "\n",
    "Po co rozróżniać pomiędzy \"B\", \"I\" i \"O\", czy nie można uwzględnić tylko dwóch tagów \"wewnątrz frazy\", \"poza frazą\"? Teoretycznie można, ale wprowadzimy w ten sposób sytuacje niejednoznaczne. \n",
    "\n",
    "Sprawdźmy to na przykładzie sekwencji \"XBox Playstation\" reprezentującej 2 osobne byty. Używając tagowania IOB nasza sekwencja wyglądałaby tak:\n",
    "\n",
    "XBox B\n",
    "PlayStation B\n",
    "\n",
    "Widzimy więc, że dwa tagi \"B\" oznaczają dwa początki osobnych fraz. Co jednak gdybyśmy używali tagów \"wewnątrz (interesującej nas) frazy\", \"poza (interesującą nas) frazą\"?\n",
    "\n",
    "XBox \"wewnątrz (interesującej nas) frazy\"\n",
    "Playstation \"wewnątrz (interesującej nas) frazy\"\n",
    "\n",
    "W tej sytuacji oznaczyliśmy poprawnie oba tokeny jako części interesujących nas fraz. Jednak nie wiemy, czy XBox Playstation to jedna, czy dwie osobne frazy (byty) -- stąd format IOB jest zdecydowanie bezpieczniejszym wyborem.\n",
    "\n",
    "**Zadanie 2b (0.5 pkt)**: Napisz funkcję, która wygeneruje CONLL z uwzględnieniem tagów IOB dotyczących urządzeń.\n",
    "Nasza funkcja posiada teraz dodatkowy argument devices, który zawiera listę obiektów, które opisują gdzie (przesunięcie znakowe) znajduje się początek i koniec wzmianek.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID\tText\tPOStag\tIOB\n",
      "1\tKate\tNNP\tO\n",
      "2\tuses\tVBZ\tO\n",
      "3\tIPhone\tNNP\tB\n",
      "4\t5\tCD\tI\n",
      "5\tand\tCC\tO\n",
      "6\tIPad\tNNP\tB\n",
      "7\t.\t.\tO\n",
      "8\tKate\tNNP\tO\n",
      "9\tbought\tVBD\tO\n",
      "10\t2\tCD\tO\n",
      "11\tlolis\tNNS\tO\n",
      "12\t.\t.\tO\n"
     ]
    }
   ],
   "source": [
    "def generate_CONLL(text, devices=[]):\n",
    "    tokenized = word_tokenize(text)\n",
    "    longest = max([len(t) for t in tokenized])\n",
    "    sen = nlp(text)\n",
    "    text_n_idx = [[token.text, token.idx] for token in sen]\n",
    "    \n",
    "    ob_flags = [\"B\" if any([d[\"begin\"] == tidx[1] for d in devices])\n",
    "                    else \"O\" for tidx in text_n_idx]\n",
    "    iob_flags = [\"I\" if any([d[\"begin\"] < tidx[1] and d[\"end\"] > tidx[1] for d in devices])\n",
    "                     else ob_flags[i] for i, tidx in enumerate(text_n_idx)]\n",
    "    \n",
    "    postags = pos_tag(word_tokenize(text))\n",
    "    print(f\"ID\\tText\\tPOStag\\tIOB\")\n",
    "    for i, (word, tag) in enumerate(postags):\n",
    "        print(f\"{i+1}\\t{word}\\t{tag}\\t{iob_flags[i]}\")\n",
    "    \n",
    "\n",
    "# parametr devices to lista słowników w którym mamy informację o numerze znaku na którym fraza się zaczyna i kończy (zobacz: próba wywołania w ostatniej linijce) (litera I z Iphone występuje na 10 znaku)\n",
    "# Zapoznaj się z dokumentacją SpaCy (obiekt Token), aby zobaczyć jak wydobyć informację o pozycji danego słowa w zdaniu/dokumencie.\n",
    "    \n",
    "generate_CONLL(\"Kate uses IPhone 5 and IPad. Kate bought 2 lolis.\", devices=[{\"begin\": 10, \"end\":18}, {\"begin\": 23, \"end\": 27}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Często chcemy w tekście naraz oznaczać byty, które należą do różnych kategorii, np. lokacje, numery telefonów, daty, wzmianki o osobach. W takich sytuacjach używa się również kodowania IOB jednak wzbogaca się etykiety o odpowiednie informacje używając formatu:\n",
    "\n",
    "{tag IOB}-{etykieta kategorii}\n",
    "\n",
    "Stąd daty przyjmują oznaczenia: B-DATE / I-DATE, osoby B-PERSON / I-PERSON, numery telefonów B-PHONENUMBER / I-PHONENUMBER, lokacje: B-LOCATION / I-LOCATION itp. Wiemy zatem czy dany token należy do interesującej nas frazy i do jakiej kategorii przypisana jest ta fraza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykrywanie encji nazwanych (Named Entity Recognition - NER)\n",
    "\n",
    "Dotychczas na większości zajęć rozważaliśmy problem klasyfikacji, w którym całym dokumentom przypisywalśmy pojedynczą etykietę (sentyment związany z dokumentem, informacja o tym, czy tekst jest spamowy, etykieta mówiąca o tym w jakim języku napisany jest dokument). Warto jednak również wspomnieć o tzw. tagowaniu sekwencji, które dla każdego elementu sekwencji (słowa) nadaje odpowiednią etykietę.\n",
    "\n",
    "Gdzie taka procedura ma zastosowanie? Wymieńmy kilka przykładów \n",
    "<ol>\n",
    "    <li>Wykrywanie wyrażeń dotyczących miejsc, ludzi, czasu, lokalizacji itp. - każde kolejne słowo tagowane jest informacją mówiącą o tym, czy dane słowo jest częścią pożądanego przez nas typu (np. częścią lokalizacji), czy nie (np. z użyciem kodowania IOB, o którym mówiliśmy przy okazji CONLL)</li>\n",
    "    <li>Tagowanie częściami mowy - każde słowo otrzymuje etykietę mówiącą o tym jaka część mowy reprezentowana jest przez aktualny token.</li>\n",
    "    <li>Wykrywanie ważnych z naszego punktu widzenia fraz (nazwy produktów, technologii itp.)</li>\n",
    "    <li>...</li>\n",
    "</ol>\n",
    "\n",
    "Mówiąc o encjach nazwanych (Named Entities) - mówimy o frazach, którym nadaliśmy określony typ, np: \"01.06.2018\" - typ \"Data\", \"Poznań, Polska\" - typ \"Lokalizacja\", \"GeForce 1080 GTX Ultra\" - typ \"Sprzęt Komputerowy\".\n",
    "\n",
    "\n",
    "\n",
    "## Własny NER - trening z użyciem algorytmu CRF (Conditional Random Fields)\n",
    "\n",
    "Wykrywacze encji wytrenowane są do odnajdywania popularnych typów fraz (Daty, Lokalizacje, Osoby, ...). Co jednak, kiedy chcielibyśmy wykrywać zdefiniowane przez nas typy danych (np. sprzęt komputerowy), które nie są domyśnie wspierane przez istniejące modele? Musielibyśmy wytrenować własnego NERa. Użyjmy paczki 'pycrfsuite' do tego celu.\n",
    "\n",
    "PyCRFSuite implementuje algorytm CRF - bardzo wydajny algorytm, który potrafi uczyć się tagowania poszczególnych słów z użyciem np. kodowania IOB. Aby rozróżnić różne rodzaje encji, często tagi \"I\" i \"B\" kodowania IOB opatruje się dodatkowym sufiksem. Np. B-Date - oznacza początek daty, a I-Location - kontynuację frazy zawierającej lokację.\n",
    "\n",
    "Ponieważ to czy dane słowo jest encją nazwaną zależy zarówno od tego jak dane słowo wygląda, jak i od słów poprzedzających i następujących po aktualnym - w opisie cech CRFów również uwzględnia się informacje o okalających słowach.\n",
    "\n",
    "**Zadanie 3 (2 punkty)** Wytrenuj model, który będzie tagował poszczególne słowa w tekście z użyciem pycfrsuite. Aby to zrobić, wykonaj podzadania w krokach poniżej.\n",
    "\n",
    "Nasz NER będzie się uczyć etykiet na zbiorze tekstów hiszpańskich, które poddane są podziałowi na zdania, tokenizacji, tagowaniem częściami mowy i etykietami encji do wykrycia w formacie IOB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('conll2002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-crfsuite in c:\\users\\asus\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (0.9.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\asus\\appdata\\local\\programs\\python\\python37-32\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('El', 'DA', 'O'),\n",
       " ('Abogado', 'NC', 'B-PER'),\n",
       " ('General', 'AQ', 'I-PER'),\n",
       " ('del', 'SP', 'I-PER'),\n",
       " ('Estado', 'NC', 'I-PER'),\n",
       " (',', 'Fc', 'O'),\n",
       " ('Daryl', 'VMI', 'B-PER'),\n",
       " ('Williams', 'NC', 'I-PER'),\n",
       " (',', 'Fc', 'O'),\n",
       " ('subrayó', 'VMI', 'O'),\n",
       " ('hoy', 'RG', 'O'),\n",
       " ('la', 'DA', 'O'),\n",
       " ('necesidad', 'NC', 'O'),\n",
       " ('de', 'SP', 'O'),\n",
       " ('tomar', 'VMN', 'O'),\n",
       " ('medidas', 'NC', 'O'),\n",
       " ('para', 'SP', 'O'),\n",
       " ('proteger', 'VMN', 'O'),\n",
       " ('al', 'SP', 'O'),\n",
       " ('sistema', 'NC', 'O'),\n",
       " ('judicial', 'AQ', 'O'),\n",
       " ('australiano', 'AQ', 'O'),\n",
       " ('frente', 'RG', 'O'),\n",
       " ('a', 'SP', 'O'),\n",
       " ('una', 'DI', 'O'),\n",
       " ('página', 'NC', 'O'),\n",
       " ('de', 'SP', 'O'),\n",
       " ('internet', 'NC', 'O'),\n",
       " ('que', 'PR', 'O'),\n",
       " ('imposibilita', 'VMI', 'O'),\n",
       " ('el', 'DA', 'O'),\n",
       " ('cumplimiento', 'NC', 'O'),\n",
       " ('de', 'SP', 'O'),\n",
       " ('los', 'DA', 'O'),\n",
       " ('principios', 'NC', 'O'),\n",
       " ('básicos', 'AQ', 'O'),\n",
       " ('de', 'SP', 'O'),\n",
       " ('la', 'DA', 'O'),\n",
       " ('Ley', 'NC', 'B-MISC'),\n",
       " ('.', 'Fp', 'O')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install python-crfsuite\n",
    "import nltk\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train')) # załaduj korpus treningowy dla języka hiszpańskiego\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))  # załaduj korpus testowy dla języka hiszpańskiego\n",
    "train_sents[2] # wyświetla przykładowe zdanie, aby zobaczyć jak reprezentowane są dane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 3a (1 punkt)** Tworzenie cech. PyCRFSuite oczekuje, że każde słowo opisane będzie zestawem odpowiednich cech w formie pythonowego słownika. Uzupełnij kod funkcji word2features (sekcje TODO) tak, aby stworzyć odpowiednie cechy zgodnie z nazwami i komentarzami do poszczególnych pól."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]  # sent[i] ma postać np. ('Ley', 'NC', 'B-MISC'); Indeks 0 oznacza pierwszy element z nawiasów (tupli), czyli w tym przypadku 'Ley'\n",
    "    postag = sent[i][1] # sent[i] ma postać np. ('Ley', 'NC', 'B-MISC'); Indeks 0 oznacza pierwszy element z nawiasów (tupli), czyli w tym przypadku 'NC'\n",
    "    \n",
    "    features = {      # cechy aktualnego słowo\n",
    "        'bias': 1.0,\n",
    "        'lowercase_word': word.lower(), \n",
    "        'word_last_3_chars': word[-3:], \n",
    "        'word_last_2_chars': word[-2:], \n",
    "        'word_is_uppercase': word.isupper(), \n",
    "        'word_is_digit': word.isnumeric(), \n",
    "        'postag': postag, \n",
    "        'postag_first_two_chars': postag[:2]   \n",
    "    }\n",
    "    if i > 0:         # jeśli nasze słowo nie jest pierwszym w zdaniu - dodajmy do zbioru naszych cech cechy poprzedniego tokenu\n",
    "        word1 = sent[i-1][0]    # poprzednie słowo\n",
    "        postag1 = sent[i-1][1]  # poprzedni pos-tag\n",
    "        \n",
    "        features.update({       # funkcja update() na słowniku dopisuje dodatkowe atrybuty do istniejącego słownika\n",
    "            'previous_word_lower': word1.lower(), \n",
    "            'previous_word_is_upppercase': word1.isupper(), \n",
    "            'previous_word_postag': postag1, \n",
    "            'previous_word_postag_first_two_chars': postag1[:2] \n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True   # jeśli to pierwszy token - ustawmy cechę BOS (Begin of Sentence) na True\n",
    "        \n",
    "    if i < len(sent)-1:          # Jeśli nasze słowo nie jest ostatnim - dodajmy do zbioru cech cechy następnego słowa \n",
    "        word1 = sent[i+1][0]     # następne słowo\n",
    "        postag1 = sent[i+1][1]   # następny postag\n",
    "        \n",
    "        features.update({        # funkcja update() na słowniku dopisuje dodatkowe atrybuty do istniejącego słownika\n",
    "            'next_word_is_lower': word1.islower(),\n",
    "            'next_word_is_upppercase': word1.isupper(), \n",
    "            'next_word_postag': postag1,  \n",
    "            'next_word_postag_first_two_chars': postag1[:2] \n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True   # jeśli to ostatni token - ustawmy cechę EOS (End of Sentence) na True\n",
    "                \n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))] # zamień każde słowo ze zdania na słownik cech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bias': 1.0,\n",
       " 'lowercase_word': 'melbourne',\n",
       " 'word_last_3_chars': 'rne',\n",
       " 'word_last_2_chars': 'ne',\n",
       " 'word_is_uppercase': False,\n",
       " 'word_is_digit': False,\n",
       " 'postag': 'NP',\n",
       " 'postag_first_two_chars': 'NP',\n",
       " 'BOS': True,\n",
       " 'next_word_is_lower': False,\n",
       " 'next_word_is_upppercase': False,\n",
       " 'next_word_postag': 'Fpa',\n",
       " 'next_word_postag_first_two_chars': 'Fp'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2features(train_sents[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oczekiwany rezultat: \n",
    "<pre>\n",
    "{'BOS': True,\n",
    " 'bias': 1.0,\n",
    " 'lowercase_word': 'melbourne',\n",
    " 'next_word_lower': 'False',\n",
    " 'next_word_is_upppercase': False,\n",
    " 'next_word_postag': 'Fpa',\n",
    " 'next_word_postag_first_two_chars': 'Fp',\n",
    " 'postag': 'NP',\n",
    " 'postag_first_two_chars': 'NP',\n",
    " 'word_is_digit': False,\n",
    " 'word_is_uppercase': False,\n",
    " 'word_last_2_chars': 'ne',\n",
    " 'word_last_3_chars': 'rne'}\n",
    "</pre>\n",
    " \n",
    " **Zadanie 3b (1 punkt) - napisz ciała funkcji pomocniczych, które dla aktualnego zdania z train_sents i test_sents zwrócą:**\n",
    " <ul>\n",
    "     <li>sent2labels - zwróci ciąg oczkiwanych etykiet dla każdego wyrazu. parametr sent jest listą słów, z których każde słowo opisane jest trójką: tekst słowa, pos-tag słowa, etykieta słowa; np. ('Abogado', 'NC', 'B-PER') </li>\n",
    "     <li>sent2tokens - analogicznie do powyższego, jednak zamiast etykiet zwróci ciąg słów bez pos-tagów i etykiet.</li>\n",
    "     <li>get_all_labels - funkcja, która ze zbioru wszystkich zdań treningowych wyświetli zbiór etykiet (zbiór, czyli bez powtórzeń). Funkcja pokaże nam ilu etykiet chcemy się nauczyć, aby móc ocenić trudność naszego problemu.</li>\n",
    " </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NP', 'Fpa', 'NP', 'Fpt', 'Fc', 'Z', 'NC', 'Fpa', 'NC', 'Fpt', 'Fp']\n",
      "['Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE', ')', '.']\n",
      "{'B-LOC', 'B-ORG', 'B-PER', 'B-MISC', 'I-PER', 'I-ORG', 'I-LOC', 'I-MISC', 'O'}\n"
     ]
    }
   ],
   "source": [
    "def sent2labels(sent):\n",
    "    return [s[1] for s in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [s[0] for s in sent]\n",
    "\n",
    "def get_all_labels(train_sents):\n",
    "    return {s[2] for sent in train_sents for s in sent}\n",
    "\n",
    "print(sent2labels(train_sents[0]))\n",
    "print(sent2tokens(train_sents[0]))\n",
    "print(get_all_labels(train_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oczekiwany rezultat:\n",
    "<pre>\n",
    "['B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n",
    "['Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE', ')', '.']\n",
    "{'I-PER', 'I-MISC', 'B-LOC', 'I-LOC', 'B-PER', 'B-MISC', 'I-ORG', 'B-ORG', 'O'}\n",
    "</pre>\n",
    "\n",
    "Uruchom poniższy kod i sprawdź czego nauczył się nasz NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La Coruña , 23 may ( EFECOM ) .\n",
      "\n",
      "Predicted: DA NC Fc Z NC Fpa NP Fpt Fp\n",
      "Correct:   DA NC Fc Z NC Fpa NP Fpt Fp\n"
     ]
    }
   ],
   "source": [
    "X_train = [sent2features(s) for s in train_sents] # Stwórz cechy zbioru treningowego\n",
    "y_train = [sent2labels(s) for s in train_sents]   # Pobierz etykiety zbioru treningowego\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]   # Stwórz cechy zbioru testowego\n",
    "y_test = [sent2labels(s) for s in test_sents]     # Pobierz etykiety zbioru testowego\n",
    "\n",
    "trainer = pycrfsuite.Trainer(verbose=False)    # stwórz obiekt trenujący\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):       # iteruj po zdaniach i etykietach\n",
    "    trainer.append(xseq, yseq)                 # dopisuj do obiektu trenującego nasze dane\n",
    "    \n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # parametr regularyzacyjny L1\n",
    "    'c2': 1e-3,  # parametr regularyzacyjny L2\n",
    "    'max_iterations': 50,  # maksymalna liczba iteracji\n",
    "    # dodaj tranzycje, które nie są obserwowane ale są możliwe\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "trainer.train('conll2002-esp.crfsuite')       # wytrenuj model i zapisz do pliku!\n",
    "\n",
    "tagger = pycrfsuite.Tagger()                  # stwórz tagger, który będzie nadawał etykiety naszej sekwencji\n",
    "tagger.open('conll2002-esp.crfsuite')         # załaduj do niego wytrenowany model\n",
    "example_sent = test_sents[0]                  # weź pierwsze z brzegu zdanie, które nie brało udziału w treningu\n",
    "print(' '.join(sent2tokens(example_sent)), end='\\n\\n')   # wyświetl je...\n",
    "\n",
    "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))  # zobacz, co generuje nasz model\n",
    "print(\"Correct:  \", ' '.join(sent2labels(example_sent)))                # i to, czego oczekiwano!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition za pomocą sieci neuronowych:\n",
    "\n",
    "Jeśli zastanawiacie się jak zrobić NERa za pomocą sieci neuronowych (Keras), to na Kaggle jest świetny fragment kodu: https://www.kaggle.com/ananysharma/ner-with-bi-lstm\n",
    "\n",
    "Po ostatnich zajęciach ten kod powinien być prosty do zrozumienia :)\n",
    "\n",
    "Bi-LSTM to dwukierunkowe LSTMy (zerknijcie na ostatnie slajdy z wprowdzenia do RNN (laboratoria 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
